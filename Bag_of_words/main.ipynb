{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "##read in data##\n",
    "################\n",
    "################\n",
    "import pandas as pd, numpy as np\n",
    "train   = pd.read_csv(\"labeledTrainData.tsv\"  , header=0, encoding='utf-8', quoting=3, delimiter='\\t')\n",
    "unlabel = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, encoding='utf-8', quoting=3, delimiter='\\t')\n",
    "test    = pd.read_csv(\"testData.tsv\"          , header=0, encoding='utf-8', quoting=3, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "###################################\n",
    "##       bag of words            ##\n",
    "###################################\n",
    "###################################\n",
    "from auxiliary import *\n",
    "\n",
    "\n",
    "def get_clean_documents(data,text_name,remove_stopwords=False):\n",
    "    clean_documents = []\n",
    "    for document in data[text_name]:\n",
    "        clean_documents.append(\" \".join(document_to_wordlist(document,remove_stopwords)))\n",
    "    return clean_documents\n",
    "        \n",
    "        \n",
    "clean_train_reviews   = get_clean_documents(train  ,'review',remove_stopwords=True)\n",
    "clean_unlabel_reviews = get_clean_documents(unlabel,'review',remove_stopwords=True)\n",
    "clean_test_reviews    = get_clean_documents(test   ,'review',remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#               Vectorizing       #\n",
    "###################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer  = TfidfVectorizer(min_df=2, max_df=0.95, max_features=500000, ngram_range=(1,4), sublinear_tf=True)\n",
    "vectorizer  = vectorizer.fit(clean_train_reviews + clean_unlabel_reviews)\n",
    "X_train_bow = vectorizer.transform(clean_train_reviews)\n",
    "X_test_bow  = vectorizer.transform(clean_test_reviews )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --------- try logistic regression ---------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(class_weight=\"auto\")\n",
    "clf.fit(X_train_bow, train['sentiment'])\n",
    "p_bow_lr = clf.predict_proba(X_test_bow)[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bow_lr} )\n",
    "output.to_csv('y_bow_lr.csv', index = False, quoting = 3 )\n",
    "# 0.96145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------- try naive bayes ---------\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha=0.0005)\n",
    "nb.fit( X_train_bow, train[\"sentiment\"] )\n",
    "p_bow_nb = nb.predict_proba( X_test_bow )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bow_nb} )\n",
    "output.to_csv('y_bow_nb.csv', index = False, quoting = 3 )\n",
    "# 0.94721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ---------- try SGD --------\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='modified_huber', n_iter=100, random_state=0, shuffle=True)\n",
    "sgd.fit( X_train_bow, train[\"sentiment\"] )\n",
    "p_bow_sgd = sgd.predict_proba( X_test_bow )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bow_sgd} )\n",
    "output.to_csv('y_bow_sgd.csv', index = False, quoting = 3 )\n",
    "# 0.96789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --------- adaboost -----------\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit( X_train_bow, train[\"sentiment\"] )\n",
    "p_bow_ada = ada.predict_proba( X_test_bow )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bow_ada} )\n",
    "output.to_csv('y_bow_ada.csv', index = False, quoting = 3 )\n",
    "# 0.90840 for y, 0.88681 for proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------- tree based ---------\n",
    "from sklearn import tree\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "tree.fit(X_train_bow, train[\"sentiment\"] )\n",
    "p_bow_tree = tree.predict_proba( X_test_bow )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bow_tree} )\n",
    "output.to_csv('y_bow_tree.csv', index = False, quoting = 3 )\n",
    "# 0.71384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#ensembling through bow#\n",
    "########################\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": \n",
    "                               -1.7*p_bow_lr + 3*p_bow_sgd + .25*p_bow_nb + 1.2*p_bow_ada + .03*p_bow_tree} ) #0.97043\n",
    "output.to_csv('y_bow.csv', index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aurora/anaconda/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/Aurora/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 empty entries\n",
      "25000 empty entries\n",
      "25000 empty entries\n",
      "25000 empty entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aurora/anaconda/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "#####################\n",
    "##  doc2vec        ##\n",
    "#####################\n",
    "#####################\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "from auxiliary import *\n",
    "n_dim = 3000\n",
    "\n",
    "model_d2v_dm = Doc2Vec.load(\"model_d2v_dm\")\n",
    "model_d2v    = Doc2Vec.load(\"model_d2v\")\n",
    "\n",
    "train_reviews = getCleanLabeledtexts(train,'review','id')\n",
    "test_reviews  = getCleanLabeledtexts(test ,'review','id')\n",
    "\n",
    "X_train_d2v_dm   = getFeatureVecs(train_reviews, model_d2v_dm, n_dim)\n",
    "X_train_d2v_dbow = getFeatureVecs(train_reviews, model_d2v, n_dim)\n",
    "\n",
    "X_test_d2v_dm    = getFeatureVecs(test_reviews, model_d2v_dm, n_dim)\n",
    "X_test_d2v_dbow  = getFeatureVecs(test_reviews, model_d2v, n_dim) \n",
    "\n",
    "X_train_d2v = np.hstack((X_train_d2v_dm, X_train_d2v_dbow))\n",
    "X_test_d2v  = np.hstack((X_test_d2v_dm, X_test_d2v_dbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------- logistic ---------\n",
    "from scipy.sparse import hstack\n",
    "clf = LogisticRegression()\n",
    "clf.fit(hstack(( X_train_bow, X_train_d2v)), train[\"sentiment\"])\n",
    "p_bdv_lr = clf.predict_proba(hstack(( X_test_bow, X_test_d2v)))[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bdv_lr} )\n",
    "output.to_csv('y_bdv_lr.csv', index = False, quoting = 3 )\n",
    "# 0.96145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------- try SGD ---------\n",
    "clf = SGDClassifier(loss='modified_huber', n_iter=100, random_state=0, shuffle=True)\n",
    "clf.fit( hstack(( X_train_bow, X_train_d2v)), train[\"sentiment\"] )\n",
    "p_bdv_sgd = clf.predict_proba( hstack(( X_test_bow, X_test_d2v)) )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bdv_sgd} )\n",
    "output.to_csv('y_bdv_sgd.csv', index = False, quoting = 3 )\n",
    "# 0.96798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------- try naive bayes ---------\n",
    "clf = MultinomialNB(alpha=0.0005)\n",
    "clf.fit( hstack(( X_train_bow, X_train_d2v)), train[\"sentiment\"] )\n",
    "p_bdv_nb = clf.predict_proba( hstack(( X_test_bow, X_test_d2v)) )[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bdv_nb} )\n",
    "output.to_csv('y_bdv_nb.csv', index = False, quoting = 3 )\n",
    "# 0.94721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "#ensemble d2v model#\n",
    "####################\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": \n",
    "                               -2.5*p_bdv_lr + 5*p_bdv_sgd + .5*p_bdv_nb  } ) #0.97013\n",
    "output.to_csv('y_bdv.csv', index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#ensemble d2v and bow#\n",
    "######################\n",
    "p_bow = -1.7*p_bow_lr + 3*p_bow_sgd + .25*p_bow_nb + 1.2*p_bow_ada + .03*p_bow_tree\n",
    "p_bdv = -2.5*p_bdv_lr + 5*p_bdv_sgd + .5*p_bdv_nb  \n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": \n",
    "                               5.5*p_bow + 1*p_bdv } ) # 0.97037\n",
    "output.to_csv('y_bow_bdv.csv', index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0 of 25000\n",
      "Text 20000 of 25000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aurora/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/Users/Aurora/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text 0 of 25000\n",
      "Text 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "#####################\n",
    "##  word2vec       ##\n",
    "#####################\n",
    "#####################\n",
    "model = Word2Vec.load(\"5000features_4minwords_10context\")\n",
    "n_dim = 5000\n",
    "\n",
    "X_train_w2v = scale(getAvgFeatureVecs(get_clean_documents(train,'review'), model, n_dim))\n",
    "X_test_w2v  = scale(getAvgFeatureVecs(get_clean_documents(test ,'review'), model, n_dim))\n",
    "\n",
    "X_train_bwv = hstack([X_train_bow, X_train_w2v])\n",
    "X_test_bwv  = hstack([X_test_bow, X_test_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------- logistic ---------\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_bwv, train[\"sentiment\"])\n",
    "p_bwv_lr = clf.predict_proba(X_test_bwv)[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bwv_lr} )\n",
    "output.to_csv('y_bwv_lr.csv', index = False, quoting = 3 )\n",
    "# 0.95714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------- try SGD ---------\n",
    "clf = SGDClassifier(loss='modified_huber', n_iter=100, random_state=0, shuffle=True)\n",
    "clf.fit(X_train_bwv, train[\"sentiment\"])\n",
    "p_bwv_sgd = clf.predict_proba(X_test_bwv)[:,1]\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": p_bwv_sgd} )\n",
    "output.to_csv('y_bwv_sgd.csv', index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "#ensemble w2v model#\n",
    "####################\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": \n",
    "                               20.5*p_bwv_lr  -1.5*p_bwv_sgd  } ) #0.95696\n",
    "output.to_csv('y_bwv.csv', index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#ensemble d2v, w2v and bow#\n",
    "###########################\n",
    "p_bwv = 20.5*p_bwv_lr -1.5*p_bwv_sgd   \n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": \n",
    "                               6.5*p_bow - 1.0*p_bdv + .1*p_bwv_sgd } ) #0.97044\n",
    "output.to_csv('y_bow_bdv_w2v.csv', index = False, quoting = 3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
