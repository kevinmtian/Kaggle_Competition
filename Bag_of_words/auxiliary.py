import numpy as npimport reimport nltkimport nltk.dataimport pandas as pdimport numpy as npfrom bs4 import BeautifulSoupfrom nltk.corpus import stopwordsfrom gensim.models.doc2vec import LabeledSentencefrom sklearn.preprocessing import scaletokenizer = nltk.data.load('tokenizers/punkt/english.pickle')   # ------------------ FOR WORD2VEC MODEL --------------------# ---------------------------------------------------------####################################################### Convert a document to a list of words              ## optionally removing stop words                     ######################################################## ---- input  example: # ---- output example:def document_to_wordlist(document, remove_stopwords=False, stops = set(stopwords.words("english"))):    # 1. Remove HTML    text_temp = BeautifulSoup(document,"lxml").get_text()    # 2. Remove non-letters    text_temp = re.sub("[^a-zA-Z]"," ", text_temp)    # 3. Convert words to lower case and split them    words = text_temp.lower().split()    # 4. Optionally remove stop words (false by default)    if remove_stopwords:        words = [w for w in words if not w in stops]    # 5. Return a list of words    return(words)                ####################################################### Split a document into sentences                    ## Each sentence is a list of words                   ## This is used for WORD2VEC model                    ######################################################## ---- input  example: # ---- output example:def document_to_sentences(document, keep_empty , tokenizer=tokenizer, remove_stopwords=False):    # 1. Use the NLTK tokenizer to split the paragraph into sentences    raw_sentences = tokenizer.tokenize(document.strip())    # 2. Loop over each sentence    sentences = []    for raw_sentence in raw_sentences:        # If a sentence is empty, skip it        if len(raw_sentence) > 0 or keep_empty == True:            sentences.append(document_to_wordlist(raw_sentence, remove_stopwords ))        else:            continue    # this returns a list of lists    return sentences                ####################################################### Split documents into sentences                     ## Each sentence is a list of words                   ## This is used for WORD2VEC model                    ######################################################## ---- input  example: # ---- output example:def documents_to_sentences(data, text_name, keep_empty , tokenizer=tokenizer, remove_stopwords=False):    sentences = []    for text in data[text_name]:        sentences += document_to_sentences(text, keep_empty ,tokenizer,remove_stopwords)    return sentences        # Then you can train a word2vec model # input is a list of sentences, each sentence is a list of words# output is a vocabulary with each word's feature vectors    ####################################################### Convert documents to lists of words                ## optionally removing stop words                     ######################################################## ---- input  example: # ---- output example:def documents_to_words(data,text_name,keep_empty = True):    clean_text = []    for text in data[text_name]:        try:            clean_text.append(document_to_wordlist(text, True))        except:            if keep_empty:                clean_text.append([])            else:                continue    return clean_text        ####################################################### For each entry, make its feature vector            ## Idea is to traverse each word for this entry       ## This is used for WORD2VEC model                    ######################################################## ---- input  example: [u'for',u'this']# ---- output example: def makeFeatureVec(document_words, model_w2v, n_dim):    featureVec = np.zeros((n_dim,),dtype="float32")    nwords = 0.    index2word_set = set(model_w2v.index2word)    for word in document_words:        if word in index2word_set:             nwords = nwords + 1.            featureVec = np.add(featureVec,model_w2v[word])    featureVec = np.divide(featureVec,nwords)    return featureVec        ####################################################### For all entries, make its feature vector           ## Idea is to use makeFeatureVec for each entry       ## This is used for WORD2VEC model                    ######################################################## ---- input  example: [[u'for',u'this'],[u'entry','a']]# ---- output example:def getAvgFeatureVecs(documents_sentences_words, model_w2v, n_dim):    counter = 0.    textFeatureVecs = np.zeros((len(documents_sentences_words),n_dim),dtype="float32")    for text in documents_sentences_words:        if counter%20000. == 0.:            print "Text %d of %d" % (counter, len(documents_sentences_words))        textFeatureVecs[counter] = makeFeatureVec(text, model_w2v, n_dim)        counter = counter + 1.    textFeatureVecs = scale(textFeatureVecs)    return textFeatureVecs                    # ------------------ FOR DOC2VEC MODEL --------------------# ---------------------------------------------------------####################################################### Convert lists of words to labeled lists of words   ## Used for doc2vec model                             #######################################################def getCleanLabeledtexts(data,text_name,id_name,tokenizer=tokenizer):    clean_texts = documents_to_sentences(data, text_name, keep_empty=True , tokenizer=tokenizer, remove_stopwords=True)    labelized = []    for i, id_label in enumerate(data[id_name]):        try:            labelized.append(LabeledSentence(clean_texts[i], [id_label]))        except:            continue    return labelized        def getFeatureVecs(labeled_train_texts, model, n_dim):    textFeatureVecs = np.zeros((len(labeled_train_texts),n_dim),dtype="float32")    counter = -1    n_empty = 0    for text in labeled_train_texts:        counter = counter + 1        try:            textFeatureVecs[counter] = np.array(model.docvecs[text.tags[0]]).reshape((1, n_dim))        except:            n_empty += 1    print str(n_empty) + ' empty entries'    return textFeatureVecsdef get_clean_documents(data,text_name,remove_stopwords=False):    clean_documents = []    for document in data[text_name]:        clean_documents.append(" ".join(document_to_wordlist(document,remove_stopwords)))    return clean_documents                