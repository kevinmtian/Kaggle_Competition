{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import urllib\n",
    "# ACCESS_KEY = \"...\"\n",
    "# SECRET_KEY = \"...\"\n",
    "# ENCODED_SECRET_KEY = urllib.quote(SECRET_KEY, \"\")\n",
    "# AWS_BUCKET_NAME = \"ruidatabricks\"\n",
    "# MOUNT_NAME = \"my-data\"\n",
    "# dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"/mnt/my-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(\"/mnt/my-data/train.csv\")\n",
    "# data = sc.textFile(\"/users/aurora/desktop/train.csv\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.cache()\n",
    "from csv import reader\n",
    "def mysplit(input):\n",
    "  temp = [line for line in reader(input)]\n",
    "  return temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parts = data.map(lambda l: mysplit([l]))\n",
    "parts.count()\n",
    "parts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "show = list(parts.first())\n",
    "print len(show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = parts.take(1)[0]\n",
    "rows = parts.filter(lambda line: line != header)\n",
    "len(rows.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "schemadata = SQLContext.getOrCreate(sqlContext).createDataFrame(rows, header)\n",
    "# schemadata = sqlContext.createDataFrame(rows, header)\n",
    "schemadata.count()\n",
    "schemadata.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "numerics = [x-1 for x in [213, 298, 299, 300, 318, 319, 320, 334, 335, 336, 369] ]\n",
    "\n",
    "characters = [x-1 for x in [2, 6, 9, 10, 11, 12, 13, 44, 45, 197, 201, 203, 208, 214, 215, 217, 218, 222, 226, 229, 230, 232, 236, 237, 239, 273, 282, 304, 324, 341, 351, 352, 353, 403, 465, 466, 492, 839, 1933]]\n",
    "\n",
    "times = [x-1 for x in [74, 76, 157, 158, 159, 160, 167, 168, 169, 170, 177, 178, 179, 180, 205]]\n",
    "\n",
    "integers = list(set(xrange(1934)) - set(numerics) - set(characters) - set(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with time data\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "months = {v.upper():int(k) for k,v in enumerate(calendar.month_abbr)}\n",
    "\n",
    "def map_to_datetime(input):\n",
    "  if input:\n",
    "    return datetime(int('20'+input[5:7]),months[input[2:5]],int(input[:2]),int(input[8:10]))\n",
    "  else:\n",
    "    return datetime(1000,1,1)\n",
    "map_date = udf(map_to_datetime, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to integer, numeric, and datetime\n",
    "def type_change(col_name, index):\n",
    "  if index in integers:\n",
    "    return col(col_name).cast('integer').alias(col_name)\n",
    "  elif index in numerics:\n",
    "    return col(col_name).cast('float').alias(col_name)\n",
    "  elif index in times:\n",
    "    return map_date(col(col_name)).alias(col_name)\n",
    "  else:\n",
    "    return col(col_name)\n",
    "\n",
    "# Build up a list of column expressions, one per column.\n",
    "#\n",
    "# This could be done in one line with a Python list comprehension, but we're keeping\n",
    "# it simple for those who don't know Python very well.\n",
    "exprs = []\n",
    "for index, col_name in enumerate(schemadata.columns):\n",
    "  exprs.append(type_change(col_name, index))\n",
    "  \n",
    "clean_df = schemadata.select(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.count()\n",
    "clean_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to numerical\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexed = clean_df\n",
    "for index in characters:\n",
    "  indexer = StringIndexer(inputCol=indexed.columns[index], outputCol=indexed.columns[index]+'i')\n",
    "  indexed = indexer.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed2 = indexed.select(*(indexed.columns[i] for i in xrange(1934) if i not in characters and i not in times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "labeled = indexed2.map(lambda x: LabeledPoint(x.target,x[1:-1])).toDF()\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "si_model = stringIndexer.fit(labeled)\n",
    "labeled = si_model.transform(labeled)\n",
    "labeled = labeled.select('indexed','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "train, val, test = labeled.randomSplit(weights=weights,seed=seed)\n",
    "train.cache()\n",
    "val.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "rf = RandomForestClassifier(numTrees=300, maxDepth=5, labelCol=\"indexed\", featuresCol=\"features\", seed=42)\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "train_pred = rf_model.transform(train)\n",
    "val_pred = rf_model.transform(val)\n",
    "test_pred = rf_model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexed\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "\n",
    "accuracy_val = evaluator.evaluate(val_pred)\n",
    "accuracy_test = evaluator.evaluate(test_pred)\n",
    "print(\"Validation Error = %g\" % (1.0 - accuracy_val))\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "Springleaf",
  "notebookId": 1820164105744209
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
